{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44787c98",
   "metadata": {},
   "source": [
    "# XGBoost - Signal vs Background Classification - gg vs gq channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cba62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.style.ATLAS)\n",
    "FilePath='/data/dust/user/vtinari/'\n",
    "\n",
    "# Scikit-learn for data preprocessing and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae19dd1",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "Following the same data approach as DNN_Classifier for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4dbaf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading data files...\n",
      "‚úÖ Data loaded successfully in 0.31 seconds\n",
      "üìä DataFrame shape: (908258, 15)\n",
      "üìä Weight array shape: (908258,)\n",
      "üìã Columns: ['beta', 'ttbar_mass_cut', 'ttbar_betaz_cut_abs', 'ttbar_pT', 'jets_per_event', 'bbbar_deltaeta', 'bbbar_deltaR', 'ttbar_deltaeta', 'ttbar_deltaR', 'ttbar_D_variable', 'ttbar_Cos_han_variable', 'ttbar_CosTstar_had', 'ttbar_CosTstar_lep', 'prod_type', 'weights_MC_NOSYS']...\n",
      "\n",
      "üìä Dataset overview:\n",
      "   Signal events (gg, type=0): 794,173\n",
      "   Background events (gq, type=1): 32,932\n",
      "   Other events (qq, type=2): 81,153\n",
      "   Total events: 908,258\n"
     ]
    }
   ],
   "source": [
    "# Load DataFrame and weights (same approach as DNN_Classifier)\n",
    "print(\"üìÅ Loading data files...\")\n",
    "\n",
    "start_time = time.time()\n",
    "df = pd.read_pickle(FilePath+'df_3classes.pkl')\n",
    "weightarr = np.load(FilePath+'weightarr.npy')\n",
    "df['weights_MC_NOSYS'] = weightarr\n",
    "loading_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Data loaded successfully in {loading_time:.2f} seconds\")\n",
    "print(f\"üìä DataFrame shape: {df.shape}\")\n",
    "print(f\"üìä Weight array shape: {weightarr.shape}\")\n",
    "print(f\"üìã Columns: {list(df.columns[:15])}...\")\n",
    "\n",
    "# Dataset overview\n",
    "print(f\"\\nüìä Dataset overview:\")\n",
    "print(f\"   Signal events (gg, type=0): {len(df[df['prod_type']==0]):,}\")\n",
    "print(f\"   Background events (gq, type=1): {len(df[df['prod_type']==1]):,}\")\n",
    "print(f\"   Other events (qq, type=2): {len(df[df['prod_type']==2]):,}\")\n",
    "print(f\"   Total events: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c262fca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Preparing binary classification dataset...\n",
      "‚úÖ Binary dataset prepared\n",
      "üìä Events after filtering: 823,775\n",
      "üìä Signal events: 790,843 (96.0%)\n",
      "üìä Background events: 32,932 (4.0%)\n",
      "üìä Removed events: 84,483 (9.3%)\n",
      "\n",
      "üìã Features for classification (13 total):\n",
      "    1. beta\n",
      "    2. ttbar_mass_cut\n",
      "    3. ttbar_betaz_cut_abs\n",
      "    4. ttbar_pT\n",
      "    5. jets_per_event\n",
      "    6. bbbar_deltaeta\n",
      "    7. bbbar_deltaR\n",
      "    8. ttbar_deltaeta\n",
      "    9. ttbar_deltaR\n",
      "   10. ttbar_D_variable\n",
      "   11. ttbar_Cos_han_variable\n",
      "   12. ttbar_CosTstar_had\n",
      "   13. ttbar_CosTstar_lep\n"
     ]
    }
   ],
   "source": [
    "# Filter negative weights and create binary classification dataset\n",
    "print(\"üîÑ Preparing binary classification dataset...\")\n",
    "\n",
    "# Remove negative weights\n",
    "mask_positive_weights = df['weights_MC_NOSYS'] > 0\n",
    "df_cut = df[mask_positive_weights]\n",
    "\n",
    "# Create binary dataset: Signal (prod_type=0) vs Background (prod_type=1) -> selecting gq channel as BCK\n",
    "mask_binary = (df_cut['prod_type'] == 0) | (df_cut['prod_type'] == 1)\n",
    "df_binary = df_cut[mask_binary]\n",
    "\n",
    "# Select physics features (first 13 columns)\n",
    "features = list(df_binary.columns[:13])\n",
    "\n",
    "print(f\"‚úÖ Binary dataset prepared\")\n",
    "print(f\"üìä Events after filtering: {len(df_binary):,}\")\n",
    "print(f\"üìä Signal events: {len(df_binary[df_binary['prod_type']==0]):,} ({len(df_binary[df_binary['prod_type']==0])/len(df_binary)*100:.1f}%)\")\n",
    "print(f\"üìä Background events: {len(df_binary[df_binary['prod_type']==1]):,} ({len(df_binary[df_binary['prod_type']==1])/len(df_binary)*100:.1f}%)\")\n",
    "print(f\"üìä Removed events: {len(df) - len(df_binary):,} ({(len(df) - len(df_binary))/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìã Features for classification ({len(features)} total):\")\n",
    "for i, feature in enumerate(features):\n",
    "    print(f\"   {i+1:2d}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd4cfe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Normalizing features with StandardScaler...\n",
      "‚úÖ Feature normalization completed\n",
      "\n",
      "üìä Normalization verification:\n",
      "   All means ‚âà 0: True\n",
      "   All stds ‚âà 1: True\n",
      "   Features range: [-5.74, 18.85]\n",
      "\n",
      "üìã Sample of normalized data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prod_type</th>\n",
       "      <th>beta</th>\n",
       "      <th>ttbar_mass_cut</th>\n",
       "      <th>ttbar_betaz_cut_abs</th>\n",
       "      <th>ttbar_pT</th>\n",
       "      <th>jets_per_event</th>\n",
       "      <th>weights_MC_NOSYS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.432665</td>\n",
       "      <td>-0.843659</td>\n",
       "      <td>-0.278448</td>\n",
       "      <td>-0.761557</td>\n",
       "      <td>-0.800939</td>\n",
       "      <td>736.417053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.052994</td>\n",
       "      <td>-0.282165</td>\n",
       "      <td>0.076421</td>\n",
       "      <td>-0.697354</td>\n",
       "      <td>1.143525</td>\n",
       "      <td>736.417053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.604497</td>\n",
       "      <td>0.196081</td>\n",
       "      <td>1.464523</td>\n",
       "      <td>3.560405</td>\n",
       "      <td>1.143525</td>\n",
       "      <td>736.417053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.125714</td>\n",
       "      <td>-1.170547</td>\n",
       "      <td>0.244189</td>\n",
       "      <td>-0.787413</td>\n",
       "      <td>-0.800939</td>\n",
       "      <td>736.417053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.284420</td>\n",
       "      <td>-0.868736</td>\n",
       "      <td>-0.181985</td>\n",
       "      <td>-0.377959</td>\n",
       "      <td>1.143525</td>\n",
       "      <td>736.417053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prod_type      beta  ttbar_mass_cut  ttbar_betaz_cut_abs  ttbar_pT  \\\n",
       "0         0 -0.432665       -0.843659            -0.278448 -0.761557   \n",
       "1         0 -0.052994       -0.282165             0.076421 -0.697354   \n",
       "2         0  1.604497        0.196081             1.464523  3.560405   \n",
       "3         0  0.125714       -1.170547             0.244189 -0.787413   \n",
       "4         0 -0.284420       -0.868736            -0.181985 -0.377959   \n",
       "\n",
       "   jets_per_event  weights_MC_NOSYS  \n",
       "0       -0.800939        736.417053  \n",
       "1        1.143525        736.417053  \n",
       "2        1.143525        736.417053  \n",
       "3       -0.800939        736.417053  \n",
       "4        1.143525        736.417053  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature normalization with StandardScaler\n",
    "print(\"üîÑ Normalizing features with StandardScaler...\")\n",
    "\n",
    "df_normalized = df_binary.copy()\n",
    "scaler = StandardScaler()\n",
    "df_normalized[features] = scaler.fit_transform(df_normalized[features])\n",
    "\n",
    "print(\"‚úÖ Feature normalization completed\")\n",
    "print(f\"\\nüìä Normalization verification:\")\n",
    "print(f\"   All means ‚âà 0: {np.allclose(df_normalized[features].mean(), 0, atol=1e-10)}\")\n",
    "print(f\"   All stds ‚âà 1: {np.allclose(df_normalized[features].std(), 1, atol=1e-10)}\")\n",
    "print(f\"   Features range: [{df_normalized[features].min().min():.2f}, {df_normalized[features].max().max():.2f}]\")\n",
    "\n",
    "# Display sample of normalized data\n",
    "print(f\"\\nüìã Sample of normalized data:\")\n",
    "display(df_normalized[['prod_type'] + features[:5] + ['weights_MC_NOSYS']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ed839",
   "metadata": {},
   "source": [
    "## Dataset Splitting\n",
    "Using 60:20:20 split strategy (Training:Validation:Testing) with stratification to preserve class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73acf328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Splitting dataset into train/validation/test sets (60:20:20)...\n",
      "‚úÖ Stratified dataset splitting completed\n",
      "\n",
      "üìä Dataset split sizes:\n",
      "   Training set:   494,265 samples (60.0%)\n",
      "   Validation set: 164,755 samples (20.0%)\n",
      "   Testing set:    164,755 samples (20.0%)\n",
      "\n",
      "üìä Class balance verification:\n",
      "   Training  : 96.0% Signal, 4.0% Background\n",
      "   Validation: 96.0% Signal, 4.0% Background\n",
      "   Test      : 96.0% Signal, 4.0% Background\n"
     ]
    }
   ],
   "source": [
    "# Split dataset with stratification to preserve class balance\n",
    "print(\"üîÑ Splitting dataset into train/validation/test sets (60:20:20)...\")\n",
    "\n",
    "# First split: 60% train, 40% temp (validation + test)\n",
    "X_train_pd, X_temp, y_train_pd, y_temp, w_train_pd, w_temp = train_test_split(\n",
    "    df_normalized[features], \n",
    "    df_normalized['prod_type'],\n",
    "    df_normalized['weights_MC_NOSYS'], \n",
    "    train_size=0.6, \n",
    "    shuffle=True, \n",
    "    random_state=1234,\n",
    "    stratify=df_normalized['prod_type']  # Preserve class balance\n",
    ")\n",
    "\n",
    "# Second split: Split 40% temp into 20% validation and 20% test\n",
    "X_test_pd, X_vali_pd, y_test_pd, y_vali_pd, w_test_pd, w_vali_pd = train_test_split(\n",
    "    X_temp, y_temp, w_temp, \n",
    "    train_size=0.5, \n",
    "    shuffle=True, \n",
    "    random_state=1234,\n",
    "    stratify=y_temp  # Preserve class balance\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Stratified dataset splitting completed\")\n",
    "print(f\"\\nüìä Dataset split sizes:\")\n",
    "print(f\"   Training set:   {X_train_pd.shape[0]:,} samples ({X_train_pd.shape[0]/len(df_normalized)*100:.1f}%)\")\n",
    "print(f\"   Validation set: {X_vali_pd.shape[0]:,} samples ({X_vali_pd.shape[0]/len(df_normalized)*100:.1f}%)\")\n",
    "print(f\"   Testing set:    {X_test_pd.shape[0]:,} samples ({X_test_pd.shape[0]/len(df_normalized)*100:.1f}%)\")\n",
    "\n",
    "# Verify class balance preservation\n",
    "print(f\"\\nüìä Class balance verification:\")\n",
    "for split_name, labels in [(\"Training\", y_train_pd), (\"Validation\", y_vali_pd), (\"Test\", y_test_pd)]:\n",
    "    signal_pct = (labels == 0).mean() * 100\n",
    "    background_pct = (labels == 1).mean() * 100\n",
    "    print(f\"   {split_name:10s}: {signal_pct:.1f}% Signal, {background_pct:.1f}% Background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "852bc509-17d7-4d2e-bdc6-efb244bac696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Rescaling weights separately for signal and background classes in each dataset split\n",
      "==========================================================================================\n",
      "üìä Dataset splits overview:\n",
      "   Training set: 494,265 samples\n",
      "   Validation set: 164,755 samples\n",
      "   Testing set: 164,755 samples\n",
      "   Total: 823,775 samples\n",
      "\n",
      "üîß TRAINING SET RESCALING:\n",
      "--------------------------------------------------\n",
      "   Signal events: 474,506\n",
      "   Background events: 19,759\n",
      "   Original signal weight sum: 349253152.000000\n",
      "   Original background weight sum: 14543342.000000\n",
      "   Rescaled signal weight sum: 1.000000\n",
      "   Rescaled background weight sum: 1.000000\n",
      "\n",
      "üîß VALIDATION SET RESCALING:\n",
      "--------------------------------------------------\n",
      "   Signal events: 158,168\n",
      "   Background events: 6,587\n",
      "   Original signal weight sum: 116417152.000000\n",
      "   Original background weight sum: 4848261.000000\n",
      "   Rescaled signal weight sum: 1.000000\n",
      "   Rescaled background weight sum: 1.000000\n",
      "\n",
      "üîß TESTING SET RESCALING:\n",
      "--------------------------------------------------\n",
      "   Signal events: 158,169\n",
      "   Background events: 6,586\n",
      "   Original signal weight sum: 116418376.000000\n",
      "   Original background weight sum: 4847604.500000\n",
      "   Rescaled signal weight sum: 1.000000\n",
      "   Rescaled background weight sum: 1.000000\n",
      "\n",
      "üìä RESCALING SUMMARY:\n",
      "==========================================================================================\n",
      "\n",
      "Training Set:\n",
      "   Signal weights - Range: [0.00000210, 0.00000211]\n",
      "   Signal weights - Mean: 0.00000211\n",
      "   Background weights - Range: [0.00005046, 0.00005075]\n",
      "   Background weights - Mean: 0.00005061\n",
      "\n",
      "Validation Set:\n",
      "   Signal weights - Range: [0.00000630, 0.00000634]\n",
      "   Signal weights - Mean: 0.00000632\n",
      "   Background weights - Range: [0.00015137, 0.00015224]\n",
      "   Background weights - Mean: 0.00015181\n",
      "\n",
      "Testing Set:\n",
      "   Signal weights - Range: [0.00000630, 0.00000634]\n",
      "   Signal weights - Mean: 0.00000632\n",
      "   Background weights - Range: [0.00015139, 0.00015226]\n",
      "   Background weights - Mean: 0.00015184\n",
      "\n",
      "‚úÖ RESCALING COMPLETED SUCCESSFULLY!\n",
      "   üéØ Each class in each dataset split now has weights that sum to 1.0\n",
      "   üî¨ This ensures balanced weighting within each class across different dataset splits\n",
      "   üìä Training, validation, and testing sets are independently normalized\n",
      "   ‚öñÔ∏è  Each class contributes equally within its respective dataset split\n"
     ]
    }
   ],
   "source": [
    "# Rescale weights separately for signal and background classes for training, validation, and testing sets\n",
    "print(\"üîÑ Rescaling weights separately for signal and background classes in each dataset split\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Function to rescale weights separately for signal and background within a dataset\n",
    "def rescale_weights_by_class(weights, labels):\n",
    "    \"\"\"\n",
    "    Rescale weights separately for signal (0) and background (1) classes\n",
    "    so that each class has weights summing to 1.0\n",
    "    \"\"\"\n",
    "    rescaled_weights =np.zeros(len(weights), dtype=np.float32)\n",
    "    \n",
    "    # Create masks for signal (0) and background (1) classes\n",
    "    signal_mask = (labels == 0)\n",
    "    background_mask = (labels == 1)\n",
    "    \n",
    "    # Calculate sum of weights for each class\n",
    "    signal_weight_sum = weights[signal_mask].sum()\n",
    "    background_weight_sum = weights[background_mask].sum()\n",
    "    \n",
    "    # Rescale weights: weight/sum(weights) for each class separately\n",
    "    if signal_weight_sum > 0:\n",
    "        rescaled_weights[signal_mask] = np.array(weights[signal_mask] / signal_weight_sum,dtype=np.float32)\n",
    "    if background_weight_sum > 0:\n",
    "        rescaled_weights[background_mask] = np.array(weights[background_mask] / background_weight_sum,dtype=np.float32)\n",
    "\n",
    "    return rescaled_weights, signal_weight_sum, background_weight_sum\n",
    "\n",
    "# Store original weights for reference\n",
    "original_train_weights = w_train_pd.copy()\n",
    "original_vali_weights = w_vali_pd.copy()\n",
    "original_test_weights = w_test_pd.copy()\n",
    "\n",
    "print(f\"üìä Dataset splits overview:\")\n",
    "print(f\"   Training set: {len(original_train_weights):,} samples\")\n",
    "print(f\"   Validation set: {len(original_vali_weights):,} samples\")\n",
    "print(f\"   Testing set: {len(original_test_weights):,} samples\")\n",
    "print(f\"   Total: {len(original_train_weights) + len(original_vali_weights) + len(original_test_weights):,} samples\")\n",
    "\n",
    "# === TRAINING SET RESCALING ===\n",
    "print(f\"\\nüîß TRAINING SET RESCALING:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Get class distribution for training set\n",
    "train_signal_mask = (y_train_pd == 0)\n",
    "train_background_mask = (y_train_pd == 1)\n",
    "\n",
    "print(f\"   Signal events: {train_signal_mask.sum():,}\")\n",
    "print(f\"   Background events: {train_background_mask.sum():,}\")\n",
    "\n",
    "# Rescale training weights\n",
    "train_weights, train_signal_orig_sum, train_background_orig_sum = rescale_weights_by_class(\n",
    "    original_train_weights, np.array(y_train_pd)\n",
    ")\n",
    "\n",
    "print(f\"   Original signal weight sum: {train_signal_orig_sum:.6f}\")\n",
    "print(f\"   Original background weight sum: {train_background_orig_sum:.6f}\")\n",
    "print(f\"   Rescaled signal weight sum: {train_weights[train_signal_mask].sum():.6f}\")\n",
    "print(f\"   Rescaled background weight sum: {train_weights[train_background_mask].sum():.6f}\")\n",
    "\n",
    "# === VALIDATION SET RESCALING ===\n",
    "print(f\"\\nüîß VALIDATION SET RESCALING:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Get class distribution for validation set\n",
    "vali_signal_mask = (y_vali_pd == 0)\n",
    "vali_background_mask = (y_vali_pd == 1)\n",
    "\n",
    "print(f\"   Signal events: {vali_signal_mask.sum():,}\")\n",
    "print(f\"   Background events: {vali_background_mask.sum():,}\")\n",
    "\n",
    "# Rescale validation weights\n",
    "vali_weights, vali_signal_orig_sum, vali_background_orig_sum = rescale_weights_by_class(\n",
    "    original_vali_weights, np.array(y_vali_pd)\n",
    ")\n",
    "\n",
    "print(f\"   Original signal weight sum: {vali_signal_orig_sum:.6f}\")\n",
    "print(f\"   Original background weight sum: {vali_background_orig_sum:.6f}\")\n",
    "print(f\"   Rescaled signal weight sum: {vali_weights[vali_signal_mask].sum():.6f}\")\n",
    "print(f\"   Rescaled background weight sum: {vali_weights[vali_background_mask].sum():.6f}\")\n",
    "\n",
    "# === TESTING SET RESCALING ===\n",
    "print(f\"\\nüîß TESTING SET RESCALING:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Get class distribution for testing set\n",
    "test_signal_mask = (y_test_pd == 0)\n",
    "test_background_mask = (y_test_pd == 1)\n",
    "\n",
    "print(f\"   Signal events: {test_signal_mask.sum():,}\")\n",
    "print(f\"   Background events: {test_background_mask.sum():,}\")\n",
    "\n",
    "# Rescale testing weights\n",
    "test_weights, test_signal_orig_sum, test_background_orig_sum = rescale_weights_by_class(\n",
    "    original_test_weights, np.array(y_test_pd)\n",
    ")\n",
    "\n",
    "print(f\"   Original signal weight sum: {test_signal_orig_sum:.6f}\")\n",
    "print(f\"   Original background weight sum: {test_background_orig_sum:.6f}\")\n",
    "print(f\"   Rescaled signal weight sum: {test_weights[test_signal_mask].sum():.6f}\")\n",
    "print(f\"   Rescaled background weight sum: {test_weights[test_background_mask].sum():.6f}\")\n",
    "\n",
    "# === SUMMARY STATISTICS ===\n",
    "print(f\"\\nüìä RESCALING SUMMARY:\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "datasets = ['Training', 'Validation', 'Testing']\n",
    "signal_masks = [train_signal_mask, vali_signal_mask, test_signal_mask]\n",
    "background_masks = [train_background_mask, vali_background_mask, test_background_mask]\n",
    "rescaled_weights = [train_weights, vali_weights, test_weights]\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    signal_mask = signal_masks[i]\n",
    "    background_mask = background_masks[i]\n",
    "    weights = rescaled_weights[i]\n",
    "    \n",
    "    print(f\"\\n{dataset} Set:\")\n",
    "    print(f\"   Signal weights - Range: [{weights[signal_mask].min():.8f}, {weights[signal_mask].max():.8f}]\")\n",
    "    print(f\"   Signal weights - Mean: {weights[signal_mask].mean():.8f}\")\n",
    "    print(f\"   Background weights - Range: [{weights[background_mask].min():.8f}, {weights[background_mask].max():.8f}]\")\n",
    "    print(f\"   Background weights - Mean: {weights[background_mask].mean():.8f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ RESCALING COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"   üéØ Each class in each dataset split now has weights that sum to 1.0\")\n",
    "print(f\"   üî¨ This ensures balanced weighting within each class across different dataset splits\")\n",
    "print(f\"   üìä Training, validation, and testing sets are independently normalized\")\n",
    "print(f\"   ‚öñÔ∏è  Each class contributes equally within its respective dataset split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bee897d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Converting to numpy arrays for XGBoost...\n",
      "‚úÖ Data conversion completed\n",
      "üìä Training data: 494265 samples, 13 features\n",
      "üìä Validation data: 164755 samples\n",
      "üìäW_vali shape: (164755,), dtype: float32\n",
      "üìä Test data: 164755 samples\n",
      "üìä Unique labels: [0 1]\n",
      "\n",
      "‚ú® Key insight: Using uniform weights instead of complex MC rescaling\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays for XGBoost compatibility\n",
    "print(\"üîÑ Converting to numpy arrays for XGBoost...\")\n",
    "\n",
    "X_train = np.array(X_train_pd, dtype=np.float32)\n",
    "X_vali = np.array(X_vali_pd, dtype=np.float32)\n",
    "X_test = np.array(X_test_pd, dtype=np.float32)\n",
    "\n",
    "y_train = np.array(y_train_pd, dtype=np.int32)\n",
    "y_vali = np.array(y_vali_pd, dtype=np.int32)\n",
    "y_test = np.array(y_test_pd, dtype=np.int32)\n",
    "\n",
    "print(\"‚úÖ Data conversion completed\")\n",
    "print(f\"üìä Training data: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"üìä Validation data: {X_vali.shape[0]} samples\")\n",
    "print(f\"üìäW_vali shape: {vali_weights.shape}, dtype: {vali_weights.dtype}\")\n",
    "print(f\"üìä Test data: {X_test.shape[0]} samples\")\n",
    "print(f\"üìä Unique labels: {np.unique(y_train)}\")\n",
    "print(f\"\\n‚ú® Key insight: Using uniform weights instead of complex MC rescaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70a6c9c",
   "metadata": {},
   "source": [
    "## XGBoost Model Configuration and Training\n",
    "Optimized parameters for achieving ~80% AUC on physics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1658e048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating XGBoost DMatrix objects...\n",
      "‚úÖ XGBoost DMatrix objects created successfully\n",
      "üìä Training DMatrix: 494265 samples, 13 features\n",
      "üìä Validation DMatrix: 164755 samples\n",
      "üìä Test DMatrix: 164755 samples\n"
     ]
    }
   ],
   "source": [
    "# Create XGBoost DMatrix objects with optimized weights\n",
    "print(\"üîÑ Creating XGBoost DMatrix objects...\")\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train,weight=train_weights, feature_names=features)\n",
    "dval = xgb.DMatrix(X_vali, label=y_vali,weight=vali_weights,  feature_names=features)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test,weight=test_weights, feature_names=features)\n",
    "\n",
    "print(\"‚úÖ XGBoost DMatrix objects created successfully\")\n",
    "print(f\"üìä Training DMatrix: {dtrain.num_row()} samples, {dtrain.num_col()} features\")\n",
    "print(f\"üìä Validation DMatrix: {dval.num_row()} samples\")\n",
    "print(f\"üìä Test DMatrix: {dtest.num_row()} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92b4ba29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuring optimized XGBoost parameters...\n",
      "‚úÖ Optimized parameters configured\n",
      "üìã Key parameters:\n",
      "   Max depth: 3\n",
      "   Learning rate: 0.3\n",
      "   Regularization: L1=0, L2=0\n",
      "   Sampling: rows=1.0, features=1.0\n"
     ]
    }
   ],
   "source": [
    "# Configure optimized XGBoost parameters for ~80% AUC performance\n",
    "print(\"üîß Configuring optimized XGBoost parameters...\")\n",
    "\n",
    "xgb_params_aggressive = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': ['auc', 'logloss'],\n",
    "    \n",
    "    # Very simple tree structure\n",
    "    'max_depth': 3,                    # Much shallower\n",
    "    'min_child_weight': 0.1,           # Very low\n",
    "    \n",
    "    # High learning rate\n",
    "    'eta': 0.3,                        # Much higher learning rate\n",
    "    'gamma': 0,                        # No regularization\n",
    "    \n",
    "    # No sampling restrictions\n",
    "    'subsample': 1.0,                  # Use all samples\n",
    "    'colsample_bytree': 1.0,           # Use all features\n",
    "    \n",
    "    # No regularization at all\n",
    "    'reg_alpha': 0,                    # No L1\n",
    "    'reg_lambda': 0,                   # No L2\n",
    "    \n",
    "    # System parameters\n",
    "    'random_state': 1234,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 2,                    \n",
    "    'tree_method': 'hist',\n",
    "}\n",
    "\n",
    "\n",
    "evallist = [(dtrain, 'train'), (dval, 'validation')]\n",
    "\n",
    "print(\"‚úÖ Optimized parameters configured\")\n",
    "print(f\"üìã Key parameters:\")\n",
    "print(f\"   Max depth: {xgb_params_aggressive['max_depth']}\")\n",
    "print(f\"   Learning rate: {xgb_params_aggressive['eta']}\")\n",
    "# print(f\"   Scale pos weight: {xgb_params_optimized['scale_pos_weight']:.3f}\")\n",
    "print(f\"   Regularization: L1={xgb_params_aggressive['reg_alpha']}, L2={xgb_params_aggressive['reg_lambda']}\")\n",
    "print(f\"   Sampling: rows={xgb_params_aggressive['subsample']}, features={xgb_params_aggressive['colsample_bytree']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36219b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training optimized XGBoost model...\n",
      "[0]\ttrain-auc:0.77324\ttrain-logloss:0.63072\tvalidation-auc:0.77289\tvalidation-logloss:0.63077\n",
      "[50]\ttrain-auc:0.80909\ttrain-logloss:0.53507\tvalidation-auc:0.80731\tvalidation-logloss:0.53672\n",
      "[100]\ttrain-auc:0.80995\ttrain-logloss:0.53391\tvalidation-auc:0.80793\tvalidation-logloss:0.53592\n",
      "[150]\ttrain-auc:0.81040\ttrain-logloss:0.53325\tvalidation-auc:0.80826\tvalidation-logloss:0.53542\n",
      "[200]\ttrain-auc:0.81077\ttrain-logloss:0.53274\tvalidation-auc:0.80837\tvalidation-logloss:0.53522\n"
     ]
    }
   ],
   "source": [
    "# Train the optimized XGBoost model\n",
    "print(\"üöÄ Training optimized XGBoost model...\")\n",
    "\n",
    "start_time = time.time()\n",
    "evals_result = {}\n",
    "\n",
    "model = xgb.train(\n",
    "    params=xgb_params_aggressive,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=500,               \n",
    "    evals=evallist,\n",
    "    early_stopping_rounds=200,        \n",
    "    verbose_eval=50,                   \n",
    "    evals_result=evals_result\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nüéØ TRAINING COMPLETED!\")\n",
    "print(f\"   ‚ö° Training time: {training_time:.2f} seconds\")\n",
    "print(f\"   üìà Best iteration: {model.best_iteration}\")\n",
    "print(f\"   üìä Best validation AUC: {model.best_score:.6f}\")\n",
    "print(f\"   üìä Final training AUC: {evals_result['train']['auc'][-1]:.6f}\")\n",
    "\n",
    "# Performance assessment\n",
    "if model.best_score > 0.85:\n",
    "    print(\"üèÜ EXCELLENT: AUC > 0.85 - Outstanding classification performance!\")\n",
    "elif model.best_score > 0.8:\n",
    "    print(\"‚úÖ VERY GOOD: AUC > 0.8 - Strong classification performance!\")\n",
    "elif model.best_score > 0.75:\n",
    "    print(\"üëç GOOD: AUC > 0.75 - Solid classification performance\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  MODERATE: AUC < 0.75 - Room for improvement\")\n",
    "\n",
    "# Save the model\n",
    "model.save_model('trained_xgb_model_clean_optimized.json')\n",
    "print(f\"\\nüíæ Model saved as: trained_xgb_model_clean_optimized.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beda30a",
   "metadata": {},
   "source": [
    "## Training Progress Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e0ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "print(\"üìà Visualizing training progress...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot AUC progression\n",
    "axes[0].plot(evals_result['train']['auc'], label='Training AUC', color='blue', linewidth=2)\n",
    "axes[0].plot(evals_result['validation']['auc'], label='Validation AUC', color='red', linewidth=2)\n",
    "axes[0].axvline(x=model.best_iteration, color='green', linestyle='--', alpha=0.7, \n",
    "                label=f'Best Iteration ({model.best_iteration})')\n",
    "axes[0].axhline(y=0.5, color='black', linestyle=':', alpha=0.5, label='Random (0.5)')\n",
    "axes[0].set_xlabel('Boosting Round')\n",
    "axes[0].set_ylabel('AUC')\n",
    "axes[0].set_title('XGBoost Training: AUC Progress')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(0.45, 1.0)\n",
    "\n",
    "# Plot Log Loss progression\n",
    "axes[1].plot(evals_result['train']['logloss'], label='Training Log Loss', color='blue', linewidth=2)\n",
    "axes[1].plot(evals_result['validation']['logloss'], label='Validation Log Loss', color='red', linewidth=2)\n",
    "axes[1].axvline(x=model.best_iteration, color='green', linestyle='--', alpha=0.7, \n",
    "                label=f'Best Iteration ({model.best_iteration})')\n",
    "axes[1].set_xlabel('Boosting Round')\n",
    "axes[1].set_ylabel('Log Loss')\n",
    "axes[1].set_title('XGBoost Training: Loss Progress')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FilePath+'Plots/XGBoost/XGB_gq_AUC_loss.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Training summary:\")\n",
    "print(f\"   Final training AUC: {evals_result['train']['auc'][-1]:.6f}\")\n",
    "print(f\"   Best validation AUC: {model.best_score:.6f}\")\n",
    "print(f\"   Optimal stopping at round: {model.best_iteration}\")\n",
    "overfitting = evals_result['train']['auc'][-1] - model.best_score\n",
    "print(f\"   Overfitting gap: {overfitting:.4f} ({'Low' if overfitting < 0.02 else 'Moderate' if overfitting < 0.05 else 'High'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced81bb",
   "metadata": {},
   "source": [
    "## Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "print(\"üìä EVALUATING MODEL ON TEST SET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Make predictions\n",
    "test_predictions_proba = model.predict(dtest, iteration_range=(0, model.best_iteration + 1))\n",
    "test_predictions = (test_predictions_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "precision = precision_score(y_test, test_predictions)\n",
    "recall = recall_score(y_test, test_predictions)\n",
    "f1 = f1_score(y_test, test_predictions)\n",
    "\n",
    "# ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, test_predictions_proba)\n",
    "test_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"üéØ TEST SET PERFORMANCE:\")\n",
    "print(f\"   Accuracy:  {accuracy:.4f} ({accuracy:.1%})\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall:    {recall:.4f}\")\n",
    "print(f\"   F1 Score:  {f1:.4f}\")\n",
    "print(f\"   ROC AUC:   {test_auc:.6f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, test_predictions)\n",
    "print(f\"\\nüìä CONFUSION MATRIX:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"Actual      Signal  Background\")\n",
    "print(f\"Signal      {cm[0,0]:6d}    {cm[0,1]:6d}\")\n",
    "print(f\"Background  {cm[1,0]:6d}    {cm[1,1]:6d}\")\n",
    "\n",
    "# Calculate detailed rates\n",
    "TP, FP, FN, TN = cm[1,1], cm[0,1], cm[1,0], cm[0,0]\n",
    "TPR = TP / (TP + FN) if (TP + FN) > 0 else 0  # Sensitivity\n",
    "TNR = TN / (TN + FP) if (TN + FP) > 0 else 0  # Specificity\n",
    "FPR = FP / (FP + TN) if (FP + TN) > 0 else 0  # False Positive Rate\n",
    "FNR = FN / (FN + TP) if (FN + TP) > 0 else 0  # False Negative Rate\n",
    "\n",
    "print(f\"\\nüìä DETAILED CLASSIFICATION RATES:\")\n",
    "print(f\"   True Positive Rate (Sensitivity):  {TPR:.4f}\")\n",
    "print(f\"   True Negative Rate (Specificity):  {TNR:.4f}\")\n",
    "print(f\"   False Positive Rate:               {FPR:.4f}\")\n",
    "print(f\"   False Negative Rate:               {FNR:.4f}\")\n",
    "\n",
    "# Performance validation\n",
    "generalization_gap = abs(model.best_score - test_auc)\n",
    "print(f\"\\nüîç MODEL VALIDATION:\")\n",
    "print(f\"   Validation AUC: {model.best_score:.6f}\")\n",
    "print(f\"   Test AUC:       {test_auc:.6f}\")\n",
    "print(f\"   Generalization gap: {generalization_gap:.4f} ({'Excellent' if generalization_gap < 0.01 else 'Good' if generalization_gap < 0.02 else 'Moderate'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f008d29",
   "metadata": {},
   "source": [
    "## Comprehensive Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization of results\n",
    "print(\"üìà Creating comprehensive results visualization...\")\n",
    "\n",
    "fig = plt.figure(figsize=(40, 25))\n",
    "gs = fig.add_gridspec(2,3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. ROC Curve\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.plot(fpr, tpr, color='darkorange', lw=3, label=f'XGBoost (AUC = {test_auc:.4f})')\n",
    "ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.7, label='Random Classifier')\n",
    "# ax1.fill_between(fpr, tpr, alpha=0.2, color='orange')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curve', fontweight='bold')\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Prediction Distribution\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "signal_proba = test_predictions_proba[y_test == 0]\n",
    "background_proba = test_predictions_proba[y_test == 1]\n",
    "\n",
    "ax2.hist(signal_proba, bins=30, alpha=0.7, color='blue', \n",
    "         label=f'Signal (Œº={signal_proba.mean():.3f})', density=True)\n",
    "ax2.hist(background_proba, bins=30, alpha=0.7, color='red', \n",
    "         label=f'Background (Œº={background_proba.mean():.3f})', density=True)\n",
    "ax2.axvline(0.5, color='black', linestyle='--', alpha=0.8, label='Decision threshold')\n",
    "ax2.set_title('Prediction Distribution', fontweight='bold')\n",
    "ax2.set_xlabel('Background Probability')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Confusion Matrix Heatmap\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax3, \n",
    "            xticklabels=['Signal', 'Background'], yticklabels=['Signal', 'Background'])\n",
    "ax3.set_title('Confusion Matrix', fontweight='bold')\n",
    "ax3.set_xlabel('Predicted')\n",
    "ax3.set_ylabel('Actual')\n",
    "\n",
    "# 4. Performance Metrics Bar Chart\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "values = [accuracy, precision, recall, f1, test_auc]\n",
    "colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightcoral', 'lightpink']\n",
    "\n",
    "bars = ax4.bar(metrics, values, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.set_title('Performance Metrics', fontweight='bold')\n",
    "ax4.set_ylabel('Score')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 5. Feature Importance\n",
    "ax5 = fig.add_subplot(gs[1, 1:])\n",
    "feature_importance = model.get_score(importance_type='weight')\n",
    "if feature_importance:\n",
    "    importance_df = pd.DataFrame(list(feature_importance.items()), columns=['feature', 'importance'])\n",
    "    importance_df = importance_df.sort_values('importance', ascending=True)\n",
    "    \n",
    "    ax5.barh(range(len(importance_df)), importance_df['importance'], color='skyblue', alpha=0.8)\n",
    "    ax5.set_yticks(range(len(importance_df)))\n",
    "    ax5.set_yticklabels(importance_df['feature'])\n",
    "    ax5.set_xlabel('Feature Importance (Weight)')\n",
    "    ax5.set_title('Feature Importance Ranking', fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle('XGBoost Clean Model - Complete Performance Analysis', fontsize=16, fontweight='bold')\n",
    "plt.savefig(FilePath+'Plots/XGBoost/XGB_gq_results.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Comprehensive analysis completed!\")\n",
    "print(f\"üéØ Final model achieves {test_auc:.1%} AUC - Ready for physics analysis!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
